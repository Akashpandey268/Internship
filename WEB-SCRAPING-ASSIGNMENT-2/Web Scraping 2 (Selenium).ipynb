{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8742abe9",
   "metadata": {},
   "source": [
    "# WEB SCRAPING – ASSIGNMENT 2\n",
    "## Instructions :\n",
    "#### 1. All the questions must be done in a single Jupyter notebook.\n",
    "#### 2. There should be proper comments in code.\n",
    "#### _____________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfeed8",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "### This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "#### Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d45caa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2523622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0743ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3b2dfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input')\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c6275ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0254e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "210fd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "job_locations=[]\n",
    "experience_required=[]\n",
    "company_names=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "af09080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "locations=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[3]/span')\n",
    "for i in locations[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "\n",
    "experiences=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[1]/span')\n",
    "for i in experiences[0:10]:\n",
    "    experience_required.append(i.text)\n",
    "\n",
    "companies=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies[0:10]:\n",
    "    company_names.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "29322eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(job_locations),len(experience_required),len(company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2b74fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>company_names</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>job_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contractual Hiring For Top MNC || Business Dat...</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HCL hiring For Data Analyst</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer Data Analyst</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Global Indian School Education Services</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Decision Science</td>\n",
       "      <td>Jana Small Finance Bank</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Programmer / Data Analyst</td>\n",
       "      <td>Frost &amp; Sullivan</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Immediate opening For Data Analyst @ Bangalore</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Optum</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(Domlur)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          job_titles  \\\n",
       "0  Contractual Hiring For Top MNC || Business Dat...   \n",
       "1                        HCL hiring For Data Analyst   \n",
       "2                              Customer Data Analyst   \n",
       "3                                   Sr. Data Analyst   \n",
       "4                    Data Analyst - Decision Science   \n",
       "5                          Programmer / Data Analyst   \n",
       "6     Immediate opening For Data Analyst @ Bangalore   \n",
       "7                             Associate Data Analyst   \n",
       "8                             Associate Data Analyst   \n",
       "9                                Senior Data Analyst   \n",
       "\n",
       "                             company_names experience_required  \\\n",
       "0                                TeamLease             5-8 Yrs   \n",
       "1                         HCL Technologies             3-8 Yrs   \n",
       "2                                   Oracle             1-3 Yrs   \n",
       "3  Global Indian School Education Services            6-11 Yrs   \n",
       "4                  Jana Small Finance Bank             3-8 Yrs   \n",
       "5                         Frost & Sullivan             3-7 Yrs   \n",
       "6                                TeamLease             4-6 Yrs   \n",
       "7                                    Optum             2-7 Yrs   \n",
       "8                                    Optum             1-4 Yrs   \n",
       "9                                 KrazyBee             3-5 Yrs   \n",
       "\n",
       "                                       job_locations  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                 Bangalore/Bengaluru, Pune, Chennai  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                          Bangalore/Bengaluru, Pune  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...  \n",
       "6                                Bangalore/Bengaluru  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9                        Bangalore/Bengaluru(Domlur)  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([job_titles,company_names,experience_required,job_locations],index=[\"job_titles\",\"company_names\",\"experience_required\",\"job_locations\"]).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99effd1f",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "### This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "891cd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "81d06f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e1fa8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "06be11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//input[@class=\"suggestor-input \"]')\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "location=driver.find_element(By.XPATH,\"//*[@id='root']/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "submit=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5e90b5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>company_names</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>job_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Opportunity on Data Science_ Python with T...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>CitiusTech</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist/AIML Engineer</td>\n",
       "      <td>upGrad</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science Consultant</td>\n",
       "      <td>ZS Associates India Pvt Ltd</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead ML Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>SMARTPADDLE TECHNOLOGY PVT. LTD.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - A.P. Maersk</td>\n",
       "      <td>Maersk</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          job_titles  \\\n",
       "0  Job Opportunity on Data Science_ Python with T...   \n",
       "1                   Assistant Manager - Data Science   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "4                       Data Scientist/AIML Engineer   \n",
       "5                            Data Science Consultant   \n",
       "6                                  Lead ML Scientist   \n",
       "7                      Tcs Hiring For Data Scientist   \n",
       "8                                Data Scientist - II   \n",
       "9                       Data Scientist - A.P. Maersk   \n",
       "\n",
       "                                 company_names experience_required  \\\n",
       "0                                Tech Mahindra             4-9 Yrs   \n",
       "1                                   CitiusTech             5-9 Yrs   \n",
       "2                                    Accenture             6-8 Yrs   \n",
       "3  NTT DATA Business Solutions Private Limited             4-9 Yrs   \n",
       "4                                       upGrad             0-2 Yrs   \n",
       "5                  ZS Associates India Pvt Ltd             3-8 Yrs   \n",
       "6                            Fractal Analytics            6-10 Yrs   \n",
       "7              TATA CONSULTANCY SERVICES (TCS)             3-8 Yrs   \n",
       "8             SMARTPADDLE TECHNOLOGY PVT. LTD.             3-6 Yrs   \n",
       "9                                       Maersk            5-10 Yrs   \n",
       "\n",
       "                                       job_locations  \n",
       "0  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...  \n",
       "1                  Bangalore/Bengaluru, Mumbai, Pune  \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...  \n",
       "3  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...  \n",
       "4  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...  \n",
       "5        Bangalore/Bengaluru, Pune, Gurgaon/Gurugram  \n",
       "6                        Bangalore/Bengaluru, Mumbai  \n",
       "7   Bangalore/Bengaluru, Chennai, Mumbai (All Areas)  \n",
       "8     Bangalore/Bengaluru, India, Mumbai (All Areas)  \n",
       "9                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles=[]\n",
    "job_locations=[]\n",
    "experience_required=[]\n",
    "company_names=[]\n",
    "titles=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "locations=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[3]/span')\n",
    "for i in locations[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "\n",
    "experiences=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[1]/span')\n",
    "for i in experiences[0:10]:\n",
    "    experience_required.append(i.text)\n",
    "\n",
    "companies=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies[0:10]:\n",
    "    company_names.append(i.text)\n",
    "print(len(job_titles),len(job_locations),len(experience_required),len(company_names))\n",
    "\n",
    "df=pd.DataFrame([job_titles,company_names,experience_required,job_locations],index=[\"job_titles\",\"company_names\",\"experience_required\",\"job_locations\"]).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764823b9",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "### You have to use the location and salary filter.\n",
    "### You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "### You have to scrape the job-title, job-location, company name, experience required.\n",
    "### The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "### The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "#### Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "443765e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "50711b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "8028bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "ea91235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//input[@class=\"suggestor-input \"]')\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "submit=driver.find_element(By.XPATH,'//div[@class=\"qsbSubmit\"]')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4cdc0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_filter=driver.find_element(By.XPATH,'//span[@title=\"3-6 Lakhs\"]')\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "114424da",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_filter=driver.find_element(By.XPATH,'//span[@title=\"Delhi / NCR\"]')\n",
    "location_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0eaffb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>company_names</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>job_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>GlobalLogic</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "      <td>Noida, Nagpur, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>R Systems International</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Predictive Analytics</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Noida, Mumbai, Chandigarh, Hyderabad/Secundera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Feedback Infra</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>4i Odc</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist, Associate</td>\n",
       "      <td>NatWest Group</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              job_titles            company_names  \\\n",
       "0                         Data Scientist              GlobalLogic   \n",
       "1        DigitalBCG GAMMA Data Scientist  Boston Consulting Group   \n",
       "2                         Data Scientist                    Optum   \n",
       "3    Data Scientist / Chat-bot Developer             Big Seo Buzz   \n",
       "4                    Lead Data Scientist  R Systems International   \n",
       "5  Data Scientist - Predictive Analytics             Confidential   \n",
       "6      Data Scientist - Engine Algorithm             Primo Hiring   \n",
       "7                         Data Scientist           Feedback Infra   \n",
       "8                         Data Scientist                   4i Odc   \n",
       "9              Data Scientist, Associate            NatWest Group   \n",
       "\n",
       "  experience_required                                      job_locations  \n",
       "0            8-10 Yrs                 Noida, Nagpur, Bangalore/Bengaluru  \n",
       "1             2-5 Yrs                     New Delhi, Bangalore/Bengaluru  \n",
       "2             2-7 Yrs                                   Gurgaon/Gurugram  \n",
       "3             3-7 Yrs  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...  \n",
       "4            7-10 Yrs                             Noida(Sector-59 Noida)  \n",
       "5             1-6 Yrs                                 (WFH during Covid)  \n",
       "6             1-3 Yrs  Noida, Mumbai, Chandigarh, Hyderabad/Secundera...  \n",
       "7             2-4 Yrs  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...  \n",
       "8             2-4 Yrs                                   Gurgaon/Gurugram  \n",
       "9             2-7 Yrs                                              Noida  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles=[]\n",
    "job_locations=[]\n",
    "experience_required=[]\n",
    "company_names=[]\n",
    "titles=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "locations=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[3]/span')\n",
    "for i in locations[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "\n",
    "experiences=driver.find_elements(By.XPATH,'//ul[@class=\"mt-7\"]/li[1]/span')\n",
    "for i in experiences[0:10]:\n",
    "    experience_required.append(i.text)\n",
    "\n",
    "companies=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies[0:10]:\n",
    "    company_names.append(i.text)\n",
    "print(len(job_titles),len(job_locations),len(experience_required),len(company_names))\n",
    "\n",
    "df=pd.DataFrame([job_titles,company_names,experience_required,job_locations],index=[\"job_titles\",\"company_names\",\"experience_required\",\"job_locations\"]).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9922a",
   "metadata": {},
   "source": [
    "## Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "### 1. Brand\n",
    "### 2. Product Description\n",
    "### 3. Price\n",
    "### The attributes which you have to scrape is ticked marked in the below image.\n",
    "### To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "#### Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a2ad2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Usefull Libraries\n",
    "\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Importing Chrome Driver\n",
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5da0e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fc81050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "\n",
    "search_input=driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_input.send_keys('sunglasses')\n",
    "\n",
    "submit=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0f6bc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# 3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "# 4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "# 5. Now scrape data from this page as usual\n",
    "# 6. Repeat this until you get data for 100 sunglasses\n",
    "\n",
    "Brand=[]\n",
    "Product_description=[]\n",
    "Price=[]\n",
    "running_discount=[]\n",
    "\n",
    "def Scraping():\n",
    "    brands=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brands:\n",
    "        Brand.append(i.text)\n",
    "\n",
    "    descriptions=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in descriptions:\n",
    "        Product_description.append(i.text)\n",
    "\n",
    "    prices=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in prices:\n",
    "        Price.append(i.text)\n",
    "\n",
    "    discounts=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]/span')\n",
    "    for i in discounts:\n",
    "        running_discount.append(i.text)\n",
    "\n",
    "print(len(Brand),len(Product_description),len(Price),len(running_discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "540534a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while len(Brand)<=100:\n",
    "    Scraping()\n",
    "    next_page=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_page.click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c8216705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(Brand),len(Product_description),len(Price),len(running_discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "41237539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>running_discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹246</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (50)</td>\n",
       "      <td>₹1,049</td>\n",
       "      <td>47% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Retro Squar...</td>\n",
       "      <td>₹415</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection Round Sunglasses (50)</td>\n",
       "      <td>₹336</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹319</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (56)</td>\n",
       "      <td>₹989</td>\n",
       "      <td>23% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹759</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Polarized Retro Square Sunglasses (47)</td>\n",
       "      <td>₹384</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹329</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Retro Squar...</td>\n",
       "      <td>₹448</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product_description   Price  \\\n",
       "0           PIRASO          UV Protection Rectangular Sunglasses (52)    ₹246   \n",
       "1    VINCENT CHASE     Polarized, UV Protection Round Sunglasses (50)  ₹1,049   \n",
       "2           AISLIN  UV Protection, Gradient Butterfly, Retro Squar...    ₹415   \n",
       "3        Rich Club                UV Protection Round Sunglasses (50)    ₹336   \n",
       "4   ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...    ₹319   \n",
       "..             ...                                                ...     ...   \n",
       "95        Fastrack             UV Protection Wayfarer Sunglasses (56)    ₹989   \n",
       "96        Fastrack      UV Protection Wayfarer Sunglasses (Free Size)    ₹759   \n",
       "97          PIRASO             Polarized Retro Square Sunglasses (47)    ₹384   \n",
       "98  kingsunglasses         UV Protection Round Sunglasses (Free Size)    ₹329   \n",
       "99          AISLIN  UV Protection, Gradient Butterfly, Retro Squar...    ₹448   \n",
       "\n",
       "   running_discount  \n",
       "0           84% off  \n",
       "1           47% off  \n",
       "2           72% off  \n",
       "3           66% off  \n",
       "4           84% off  \n",
       "..              ...  \n",
       "95          23% off  \n",
       "96          15% off  \n",
       "97          84% off  \n",
       "98          79% off  \n",
       "99          70% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Brand,Product_description,Price,running_discount],index=['Brand','Product_description','Price','running_discount']).transpose().head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b74da",
   "metadata": {},
   "source": [
    "## Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "### This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "### You will reach to the below shown webpage . As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "#### Note: All the steps required during scraping should be done through code only and not manually.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "c650ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Usefull Libraries\n",
    "\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Importing Chrome Driver\n",
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "ec269693",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=bb943e69-64c5-485d-a9e8-afad8a445409.MOBFWQ6BXGJCEYNY.SEARCH&ppt=hp&ppn=homepage&ssid=o87tskpvy80000001662929996425&qH=f6cdfdaa9f3c23f3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "573f1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_review=driver.find_element(By.XPATH,'//div[@class=\"_3UAT2v _16PBlm\"]')\n",
    "more_review.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "722f01bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "Rating=[]\n",
    "Review_summary=[]\n",
    "Full_review=[]\n",
    "\n",
    "def Scraping():\n",
    "    ratings=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in ratings:\n",
    "        Rating.append(i.text)\n",
    "\n",
    "    summeries=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in summeries:\n",
    "        Review_summary.append(i.text)\n",
    "\n",
    "    reviews=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]/div/div')\n",
    "    for i in reviews:\n",
    "        Full_review.append(i.text)\n",
    "\n",
    "\n",
    "print(len(Rating),len(Review_summary),len(Full_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "aba0290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping()\n",
    "next_page=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\")\n",
    "next_page.click()\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "53375d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(Rating)<=100:\n",
    "    Scraping()\n",
    "    next_page=driver.find_element(By.XPATH,'//nav[@class=\"yFHi8N\"]/a[12]')\n",
    "    next_page.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7e8ff7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_summary</th>\n",
       "      <th>Full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Damn this phone is a blast . Upgraded from and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Worth the money’ starting first from its perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Absolute rubbish!</td>\n",
       "      <td>Worst product delivered by Flipkart\\nAfter 10d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>I dreamt about this day from a long time.... G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Value-for-money</td>\n",
       "      <td>I'm Really happy with the product\\nDelivery wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating       Review_summary  \\\n",
       "0       5       Simply awesome   \n",
       "1       5     Perfect product!   \n",
       "2       5  Best in the market!   \n",
       "3       5    Worth every penny   \n",
       "4       5   Highly recommended   \n",
       "..    ...                  ...   \n",
       "95      5  Best in the market!   \n",
       "96      5     Perfect product!   \n",
       "97      4    Absolute rubbish!   \n",
       "98      5              Awesome   \n",
       "99      5      Value-for-money   \n",
       "\n",
       "                                          Full_review  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Previously I was using one plus 3t it was a gr...  \n",
       "4   What a camera .....just awesome ..you can feel...  \n",
       "..                                                ...  \n",
       "95  Damn this phone is a blast . Upgraded from and...  \n",
       "96  Worth the money’ starting first from its perfo...  \n",
       "97  Worst product delivered by Flipkart\\nAfter 10d...  \n",
       "98  I dreamt about this day from a long time.... G...  \n",
       "99  I'm Really happy with the product\\nDelivery wa...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Rating,Review_summary,Full_review],index=['Rating','Review_summary','Full_review']).transpose().head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796c9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48cf45d",
   "metadata": {},
   "source": [
    "## Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "### You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "#### As shown in the below image, you have to scrape the tick marked attributes.\n",
    "#### Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "95b7591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Usefull Libraries\n",
    "\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Importing Chrome Driver\n",
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "40c3fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e4020a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search.send_keys('sneakers')\n",
    "click_search=driver.find_element(By.XPATH,'/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "click_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "8746a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "Brand=[]\n",
    "Product_description=[]\n",
    "Price=[]\n",
    "Discount=[]\n",
    "\n",
    "def Scraping():\n",
    "    brands=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brands:\n",
    "        Brand.append(i.text)\n",
    "\n",
    "    descriptions=driver.find_elements(By.XPATH,'//div[@class=\"_2B099V\"]/a[1]')\n",
    "    for i in descriptions:\n",
    "        Product_description.append(i.text)\n",
    "\n",
    "    prices=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in prices:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    discounts=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]/span')\n",
    "    for i in discounts:\n",
    "        Discount.append(i.text)\n",
    "\n",
    "\n",
    "print(len(Brand),len(Product_description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a7db0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping()\n",
    "next_page=driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "next_page.click()\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "0f6ebee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(Brand)<=100:\n",
    "    Scraping()\n",
    "    next_page=driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]')\n",
    "    next_page.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c6964fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 119\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),len(Product_description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "7ebb7866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Myshtezia</td>\n",
       "      <td>Stylish Sneaker For Women Sneakers For Women</td>\n",
       "      <td>₹299</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TR</td>\n",
       "      <td>Casual White Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹329</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>₹498</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Lightweight Pack Of 1 Trendy Sneakers Sneakers...</td>\n",
       "      <td>₹179</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Comfortable &amp; Ultra Light Weight Sneaker Sneak...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Modern Stylish Combo Pack of 3 Jogging, Walkin...</td>\n",
       "      <td>₹568</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>otoos</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹607</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>Original Luxury Branded Fashionable Men's Casu...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>HOCKWOOD</td>\n",
       "      <td>Street Smart Sneakers For Men</td>\n",
       "      <td>₹745</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand                                Product_description Price  \\\n",
       "0   Myshtezia       Stylish Sneaker For Women Sneakers For Women  ₹299   \n",
       "1          TR                      Casual White Sneakers For Men  ₹299   \n",
       "2      Labbin                                   Sneakers For Men  ₹329   \n",
       "3      Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...  ₹498   \n",
       "4      BRUTON  Lightweight Pack Of 1 Trendy Sneakers Sneakers...  ₹179   \n",
       "..        ...                                                ...   ...   \n",
       "95   LE GREEM  Comfortable & Ultra Light Weight Sneaker Sneak...  ₹449   \n",
       "96     Chevit  Modern Stylish Combo Pack of 3 Jogging, Walkin...  ₹568   \n",
       "97      otoos                                   Sneakers For Men  ₹607   \n",
       "98   ASTEROID  Original Luxury Branded Fashionable Men's Casu...  ₹449   \n",
       "99   HOCKWOOD                      Street Smart Sneakers For Men  ₹745   \n",
       "\n",
       "   Discount  \n",
       "0   85% off  \n",
       "1   80% off  \n",
       "2   67% off  \n",
       "3   73% off  \n",
       "4   70% off  \n",
       "..      ...  \n",
       "95  55% off  \n",
       "96  76% off  \n",
       "97  69% off  \n",
       "98  55% off  \n",
       "99  25% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Brand,Product_description,Price,Discount],index=['Brand','Product_description','Price','Discount']).transpose().head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97275",
   "metadata": {},
   "source": [
    "## Q7: Go to the link - https://www.myntra.com/shoes\n",
    "### Set second Price filter and Color filter to “Black”, as shown in the below image. And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "#### Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3560c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Usefull Libraries\n",
    "\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Importing Chrome Driver\n",
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5381e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.myntra.com/shoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90387f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price filter\n",
    "price_filter=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label')\n",
    "price_filter.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06e15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Black Filter\n",
    "color_filter=driver.find_element(By.XPATH,'//*[@id=\"mountRoot\"]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "color_filter.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b5ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Product_description=[]\n",
    "Price=[]\n",
    "\n",
    "\n",
    "def Scraping():\n",
    "    brands=driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    for i in brands:\n",
    "        Brand.append(i.text)\n",
    "\n",
    "    descriptions=driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    for i in descriptions:\n",
    "        Product_description.append(i.text)\n",
    "\n",
    "    prices=driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]/span[1]')\n",
    "    for i in prices:\n",
    "        Price.append(i.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98446c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while len(Brand)<=100:\n",
    "    Scraping()\n",
    "    next_page=driver.find_element(By.XPATH,'//a[@rel=\"next\"]')\n",
    "    next_page.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c0bddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM WINFLO8 Running Shoes</td>\n",
       "      <td>Rs. 7880Rs. 8295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Sports Shoes</td>\n",
       "      <td>Rs. 8499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Niteball II Sneakers</td>\n",
       "      <td>Rs. 9349Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men React Infinity 3 Running</td>\n",
       "      <td>Rs. 13295Rs. 13995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Leather Niteball Sneakers</td>\n",
       "      <td>Rs. 10799Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women Solar Glide 5 Running</td>\n",
       "      <td>Rs. 9799Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women SL20.3 Running Shoes</td>\n",
       "      <td>Rs. 7799Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>W omen TriBase Reign 4 Running</td>\n",
       "      <td>Rs. 8399Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women Adizero Adios 7 Run Shoe</td>\n",
       "      <td>Rs. 9599Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>Men Textured Leather Loafers</td>\n",
       "      <td>Rs. 7990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand             Product_description               Price\n",
       "0               Nike  Men ZOOM WINFLO8 Running Shoes    Rs. 7880Rs. 8295\n",
       "1           Skechers                Men Sports Shoes            Rs. 8499\n",
       "2   ADIDAS Originals        Men Niteball II Sneakers   Rs. 9349Rs. 10999\n",
       "3               Nike    Men React Infinity 3 Running  Rs. 13295Rs. 13995\n",
       "4   ADIDAS Originals   Men Leather Niteball Sneakers  Rs. 10799Rs. 11999\n",
       "..               ...                             ...                 ...\n",
       "95            ADIDAS     Women Solar Glide 5 Running   Rs. 9799Rs. 13999\n",
       "96            ADIDAS      Women SL20.3 Running Shoes   Rs. 7799Rs. 11999\n",
       "97      UNDER ARMOUR  W omen TriBase Reign 4 Running   Rs. 8399Rs. 11999\n",
       "98            ADIDAS  Women Adizero Adios 7 Run Shoe   Rs. 9599Rs. 11999\n",
       "99         J.FONTINI    Men Textured Leather Loafers            Rs. 7990\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Brand,Product_description,Price],index=['Brand','Product_description','Price']).transpose().head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53140a57",
   "metadata": {},
   "source": [
    "## Q8: Go to webpage https://www.amazon.in/\n",
    "### Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” as shown in the below image: After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "#### Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b3b99034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "573820ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a81f07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c28f2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input=driver.find_element(By.XPATH,'//*[@id=\"twotabsearchtextbox\"]')\n",
    "search_input.send_keys('Laptop')\n",
    "submit=driver.find_element(By.XPATH,'//span[@class=\"nav-search-submit-text nav-sprite nav-progressive-attribute\"]')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9ca716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_i7=driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span')\n",
    "core_i7.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "24b392c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8596d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in titles[0:10]:\n",
    "    Title.append(i.text)\n",
    "\n",
    "ratings=driver.find_elements(By.XPATH,'//div[@class=\"a-row a-size-small\"]/span[1]')\n",
    "for i in ratings[0:10]:\n",
    "    Rating.append(i.get_attribute('aria-label'))\n",
    "\n",
    "prices=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in prices[0:10]:\n",
    "    Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5b2ab363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Title),len(Rating),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "86baa655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>1,06,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...</td>\n",
       "      <td>3.9 out of 5 stars</td>\n",
       "      <td>72,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>3.8 out of 5 stars</td>\n",
       "      <td>57,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) Lenovo Intel Core i7 5600U 12.5-Inch...</td>\n",
       "      <td>3.3 out of 5 stars</td>\n",
       "      <td>23,997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>80,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>85,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS TUF Gaming F17 (2022), 17.3-inch (43.94 c...</td>\n",
       "      <td>3.9 out of 5 stars</td>\n",
       "      <td>1,32,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Renewed) HP Elitebook 840G1-i7-8 GB-2 TB 14-i...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>41,690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS VivoBook K15 OLED (2021), 15.6-inch FHD O...</td>\n",
       "      <td>3.7 out of 5 stars</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title              Rating  \\\n",
       "0  Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...  4.3 out of 5 stars   \n",
       "1  Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...  4.4 out of 5 stars   \n",
       "2  Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...  3.9 out of 5 stars   \n",
       "3  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...  3.8 out of 5 stars   \n",
       "4  (Renewed) Lenovo Intel Core i7 5600U 12.5-Inch...  3.3 out of 5 stars   \n",
       "5  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  4.3 out of 5 stars   \n",
       "6  Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...  4.1 out of 5 stars   \n",
       "7  ASUS TUF Gaming F17 (2022), 17.3-inch (43.94 c...  3.9 out of 5 stars   \n",
       "8  (Renewed) HP Elitebook 840G1-i7-8 GB-2 TB 14-i...  4.1 out of 5 stars   \n",
       "9  ASUS VivoBook K15 OLED (2021), 15.6-inch FHD O...  3.7 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0    82,990  \n",
       "1  1,06,999  \n",
       "2    72,890  \n",
       "3    57,990  \n",
       "4    23,997  \n",
       "5    80,990  \n",
       "6    85,800  \n",
       "7  1,32,990  \n",
       "8    41,690  \n",
       "9    82,990  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Title,Rating,Price],index=['Title','Rating','Price']).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19399942",
   "metadata": {},
   "source": [
    "## Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "### This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "#### Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f508e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "61253717",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b1706c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.ambitionbox.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b110dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_btn=driver.find_element(By.XPATH,'//*[@id=\"ambitionbox-header\"]/nav/ul/li[5]/a')\n",
    "jobs_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "919dd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//*[@id=\"jobs-typeahead\"]/span/input')\n",
    "designation.send_keys('Data Scientist')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bb89a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit=driver.find_element(By.XPATH,'//*[@id=\"jobs\"]/div[2]/div[1]/div[1]/div/div/div/button')\n",
    "submit.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7bcaca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_btn=driver.find_element(By.XPATH,'//*[@id=\"filters-row\"]/div/div/div[2]/div[1]')\n",
    "location_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f599217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "search.send_keys('Noida')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1ac61f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_btn=driver.find_element(By.XPATH,'//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "click_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a6f5933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name=[]\n",
    "number_days_ago=[]\n",
    "company_rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e38ee4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=driver.find_elements(By.XPATH,'//p[@class=\"company body-medium\"]')\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "days_ago=driver.find_elements(By.XPATH,'//*[@id=\"jobsList\"]/div[2]/div[2]/div/div//div/div[3]/span[1]')\n",
    "for i in days_ago:\n",
    "    number_days_ago.append(i.text)\n",
    "\n",
    "ratings=driver.find_elements(By.XPATH,'//*[@id=\"jobsList\"]/div[2]//div[@class=\"info\"]//span[@class=\"body-small\"]')\n",
    "for i in ratings:\n",
    "    company_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "851fe7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(company_name),len(number_days_ago),len(company_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "96ac5353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>number_days_ago</th>\n",
       "      <th>company_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BARCLAYS GLOBAL SERVICE CENTRE PRIVATE LIMITED</td>\n",
       "      <td>2d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EY GDS</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLOBALLOGIC INDIA PRIVATE LIMITED</td>\n",
       "      <td>2d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBRE South Asia Pvt Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>24d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>26d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dew Solutions Pvt. Ltd.</td>\n",
       "      <td>9d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One97 Communications Limited</td>\n",
       "      <td>17d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     company_name number_days_ago  \\\n",
       "0  Optum Global Solutions (India) Private Limited          4d ago   \n",
       "1  BARCLAYS GLOBAL SERVICE CENTRE PRIVATE LIMITED          2d ago   \n",
       "2                                          EY GDS          4d ago   \n",
       "3               GLOBALLOGIC INDIA PRIVATE LIMITED          2d ago   \n",
       "4                         CBRE South Asia Pvt Ltd        1mon ago   \n",
       "5                   GENPACT India Private Limited         24d ago   \n",
       "6                                         Genpact         26d ago   \n",
       "7        Ericsson India Global Services Pvt. Ltd.        1mon ago   \n",
       "8                         Dew Solutions Pvt. Ltd.          9d ago   \n",
       "9                    One97 Communications Limited         17d ago   \n",
       "\n",
       "  company_rating  \n",
       "0            4.1  \n",
       "1            4.3  \n",
       "2            3.8  \n",
       "3            4.0  \n",
       "4            4.3  \n",
       "5            4.0  \n",
       "6            4.0  \n",
       "7            4.3  \n",
       "8            4.3  \n",
       "9            3.8  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([company_name,number_days_ago,company_rating],index=['company_name','number_days_ago','company_rating']).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6812ad",
   "metadata": {},
   "source": [
    "## Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "### The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe.\n",
    "#### Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4f04a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dc90c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "190a3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.ambitionbox.com/')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9124e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_click=driver.find_element(By.XPATH,'//li[@class=\"navItem\"][3]')\n",
    "salary_click.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ab4844e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "brows_salary=driver.find_element(By.XPATH,'//*[@id=\"ambitionbox-header\"]/nav/ul/li[3]/div/ul/li[1]/div/div[2]/a')\n",
    "brows_salary.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e18ee027",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//*[@id=\"jobProfileSearchbox\"]')\n",
    "designation.send_keys('Data Scientist')\n",
    "time.sleep(2)\n",
    "submit=driver.find_element(By.XPATH,'//*[@id=\"salaries\"]/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div')\n",
    "submit.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6b3fd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_name=[]\n",
    "Number_of_salaries=[]\n",
    "Average_salary=[]\n",
    "Min_salary=[]\n",
    "Max_Salary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1ab10819",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]//a')\n",
    "for i in companies:\n",
    "    Company_name.append(i.text)\n",
    "\n",
    "\n",
    "salaries=driver.find_elements(By.XPATH,'//span[@class=\"datapoints\"]')\n",
    "for i in salaries:\n",
    "    Number_of_salaries.append(i.text.replace('based on ',''))\n",
    "\n",
    "\n",
    "avg_salary=driver.find_elements(By.XPATH,'//p[@class=\"averageCtc\"]')\n",
    "for i in avg_salary:\n",
    "    Average_salary.append(i.text.strip('\\nData Scientist Salary'))\n",
    "\n",
    "\n",
    "min_salaries=driver.find_elements(By.XPATH,'//div[@class=\"salary-values\"]/div[1]')\n",
    "for i in min_salaries:\n",
    "    Min_salary.append(i.text)\n",
    "\n",
    "\n",
    "max_salaries=driver.find_elements(By.XPATH,'//div[@class=\"salary-values\"]/div[2]')\n",
    "for i in max_salaries:\n",
    "    Max_Salary.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2a79769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Company_name),len(Number_of_salaries),len(Average_salary),len(Min_salary),len(Max_Salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0a7c482d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Number_of_salaries</th>\n",
       "      <th>Average_salary</th>\n",
       "      <th>Min_salary</th>\n",
       "      <th>Max_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nData Scientist Salary</td>\n",
       "      <td>(23 salaries)</td>\n",
       "      <td>₹ 32.3L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nData Scientist Salary</td>\n",
       "      <td>(57 salaries)</td>\n",
       "      <td>₹ 19.9L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 26.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum\\nData Scientist Salary</td>\n",
       "      <td>(49 salaries)</td>\n",
       "      <td>₹ 16.4L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nData Scientist Salary</td>\n",
       "      <td>(34 salaries)</td>\n",
       "      <td>₹ 15.8L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics\\nData Scientist Salary</td>\n",
       "      <td>(115 salaries)</td>\n",
       "      <td>₹ 15.4L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiger Analytics\\nData Scientist Salary</td>\n",
       "      <td>(68 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sigmoid Analytics\\nData Scientist Salary</td>\n",
       "      <td>(10 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 19.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Legato Health Technologies\\nData Scientist Salary</td>\n",
       "      <td>(11 salaries)</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HSBC\\nData Scientist Salary</td>\n",
       "      <td>(10 salaries)</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 18.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tredence\\nData Scientist Salary</td>\n",
       "      <td>(14 salaries)</td>\n",
       "      <td>₹ 13.9L</td>\n",
       "      <td>₹ 8.8L</td>\n",
       "      <td>₹ 17.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Company_name Number_of_salaries  \\\n",
       "0                     Walmart\\nData Scientist Salary      (23 salaries)   \n",
       "1                    Ab Inbev\\nData Scientist Salary      (57 salaries)   \n",
       "2                       Optum\\nData Scientist Salary      (49 salaries)   \n",
       "3                          ZS\\nData Scientist Salary      (34 salaries)   \n",
       "4           Fractal Analytics\\nData Scientist Salary     (115 salaries)   \n",
       "5             Tiger Analytics\\nData Scientist Salary      (68 salaries)   \n",
       "6           Sigmoid Analytics\\nData Scientist Salary      (10 salaries)   \n",
       "7  Legato Health Technologies\\nData Scientist Salary      (11 salaries)   \n",
       "8                        HSBC\\nData Scientist Salary      (10 salaries)   \n",
       "9                    Tredence\\nData Scientist Salary      (14 salaries)   \n",
       "\n",
       "  Average_salary Min_salary Max_Salary  \n",
       "0        ₹ 32.3L    ₹ 25.0L    ₹ 45.0L  \n",
       "1        ₹ 19.9L    ₹ 15.0L    ₹ 26.0L  \n",
       "2        ₹ 16.4L    ₹ 11.0L    ₹ 22.6L  \n",
       "3        ₹ 15.8L    ₹ 11.0L    ₹ 22.0L  \n",
       "4        ₹ 15.4L     ₹ 9.0L    ₹ 23.0L  \n",
       "5        ₹ 14.7L     ₹ 9.0L    ₹ 20.0L  \n",
       "6        ₹ 14.7L    ₹ 12.7L    ₹ 19.7L  \n",
       "7        ₹ 14.5L    ₹ 11.0L    ₹ 20.0L  \n",
       "8        ₹ 14.0L    ₹ 12.0L    ₹ 18.0L  \n",
       "9        ₹ 13.9L     ₹ 8.8L    ₹ 17.5L  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([Company_name,Number_of_salaries,Average_salary,Min_salary,Max_Salary],index=['Company_name','Number_of_salaries','Average_salary','Min_salary','Max_Salary']).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdfd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
